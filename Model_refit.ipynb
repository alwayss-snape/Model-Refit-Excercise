{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data created and split into train and test sets.\n",
      "Train set shape: (700, 20)\n",
      "Test set shape: (300, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create synthetic dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Split into train and test sets (70:30 ratio)\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop(columns=['target']).values\n",
    "y_train = train_df['target'].values\n",
    "X_test = test_df.drop(columns=['target']).values\n",
    "y_test = test_df['target'].values\n",
    "\n",
    "print(\"Sample data created and split into train and test sets.\")\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, accuracy_score\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0)\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    gbm = lgb.train(param, train_data)\n",
    "\n",
    "    y_pred = gbm.predict(X_test)\n",
    "    roc = roc_auc_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "\n",
    "    return roc, accuracy\n",
    "\n",
    "study = optuna.create_study(directions=[\"maximize\", \"maximize\"])\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "df1 = study.trials_dataframe()\n",
    "df1.columns = ['iteration_number', 'value', 'datetime_start', 'datetime_complete', 'params_learning_rate',\n",
    "               'params_num_leaves', 'params_max_depth', 'params_feature_fraction', 'params_bagging_fraction',\n",
    "               'user_attrs_roc', 'user_attrs_accuracy']\n",
    "df1 = df1.drop(columns=['datetime_start', 'datetime_complete'])\n",
    "\n",
    "print(\"Optuna tuning results stored in df1:\")\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Creating df2 with top 5 iterations based on RoC and Accuracy\n",
    "df2 = df1.nlargest(5, ['user_attrs_roc', 'user_attrs_accuracy'])\n",
    "print(\"DataFrame df2 (Top 5 iterations):\")\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Training models for top 5 iterations\n",
    "def train_model(params):\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    model = lgb.train(params, train_data)\n",
    "    return model\n",
    "\n",
    "models = []\n",
    "for _, row in df2.iterrows():\n",
    "    params = {\n",
    "        'learning_rate': row['params_learning_rate'],\n",
    "        'num_leaves': row['params_num_leaves'],\n",
    "        'max_depth': row['params_max_depth'],\n",
    "        'feature_fraction': row['params_feature_fraction'],\n",
    "        'bagging_fraction': row['params_bagging_fraction'],\n",
    "        'objective': 'binary'\n",
    "    }\n",
    "    model = train_model(params)\n",
    "    models.append(model)\n",
    "\n",
    "print(\"Models trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Calculating additional metrics for each model\n",
    "final_df = df2.copy()\n",
    "final_df['f1_score'] = None\n",
    "final_df['recall'] = None\n",
    "final_df['precision'] = None\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_pred = model.predict(X_train)\n",
    "    y_pred_class = (y_pred > 0.5).astype(int)\n",
    "    final_df.at[i, 'f1_score'] = f1_score(y_train, y_pred_class)\n",
    "    final_df.at[i, 'recall'] = recall_score(y_train, y_pred_class)\n",
    "    final_df.at[i, 'precision'] = precision_score(y_train, y_pred_class)\n",
    "\n",
    "print(\"Additional metrics calculated and stored in final_df.\")\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Calculating cumulative capture for top 3 deciles\n",
    "def calculate_cumulative_capture_top_3_deciles(y_true, y_pred):\n",
    "    preds_df = pd.DataFrame({'pred': y_pred, 'actual': y_true})\n",
    "    preds_df['Decile_rank'] = pd.qcut(preds_df['pred'].rank(method='first'), 10, labels=False)\n",
    "    \n",
    "    responses = preds_df.groupby('Decile_rank', as_index=False).agg(\n",
    "        TOTAL_COUNT=('pred', 'count'),\n",
    "        TOTAL_ACTUAL=('actual', 'sum'),\n",
    "        MEAN_PROB=('pred', 'mean')\n",
    "    )\n",
    "    \n",
    "    responses[\"ACTUAL_RR\"] = (responses[\"TOTAL_ACTUAL\"] / responses[\"TOTAL_COUNT\"]) * 100\n",
    "    responses[\"%_ACTUAL_RC\"] = (responses[\"TOTAL_ACTUAL\"] / responses[\"TOTAL_ACTUAL\"].sum()) * 100\n",
    "    responses[\"CUMULATED_RC\"] = responses['%_ACTUAL_RC'][::-1].cumsum()[::-1]\n",
    "    \n",
    "    top_3_deciles_cumulative = responses[responses['Decile_rank'] < 3]['CUMULATED_RC'].sum()\n",
    "    return top_3_deciles_cumulative\n",
    "\n",
    "final_df['cumulative_capture_top_3_Deciles'] = None\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_pred = model.predict(X_train)\n",
    "    final_df.at[i, 'cumulative_capture_top_3_Deciles'] = calculate_cumulative_capture_top_3_deciles(y_train, y_pred)\n",
    "\n",
    "print(\"Cumulative capture for top 3 deciles calculated and stored in final_df.\")\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Finding decile break\n",
    "def find_decile_break(y_true, y_pred):\n",
    "    preds_df = pd.DataFrame({'pred': y_pred, 'actual': y_true})\n",
    "    preds_df['Decile_rank'] = pd.qcut(preds_df['pred'].rank(method='first'), 10, labels=False)\n",
    "    \n",
    "    responses = preds_df.groupby('Decile_rank', as_index=False).agg(\n",
    "        TOTAL_COUNT=('pred', 'count'),\n",
    "        TOTAL_ACTUAL=('actual', 'sum')\n",
    "    )\n",
    "    \n",
    "    decile_break = None\n",
    "    \n",
    "    for i in range(1, 10):\n",
    "        if responses.at[i, 'TOTAL_ACTUAL'] == 0:\n",
    "            decile_break = i\n",
    "            break\n",
    "\n",
    "    return decile_break\n",
    "\n",
    "final_df['decile_break'] = None\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_pred = model.predict(X_train)\n",
    "    final_df.at[i, 'decile_break'] = find_decile_break(y_train, y_pred)\n",
    "\n",
    "print(\"Decile break calculated and stored in final_df.\")\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Adding feature importance as Gain percentages\n",
    "feature_names = [f'feature_{i}' for i in range(20)]\n",
    "\n",
    "# Initialize columns in final_df for feature importances as percentages\n",
    "for feature in feature_names:\n",
    "    final_df[feature] = None\n",
    "\n",
    "# Adding feature importance for each model as Gain percentages\n",
    "for i, model in enumerate(models):\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    importance_percentage = (importance / importance.sum()) * 100\n",
    "    for j, feature in enumerate(feature_names):\n",
    "        final_df.at[i, feature] = f\"{importance_percentage[j]:.2f}%\"\n",
    "\n",
    "print(\"Feature importance as Gain percentages calculated and stored in final_df.\")\n",
    "print(final_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
